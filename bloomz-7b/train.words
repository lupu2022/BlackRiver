%def create_var
    1 "cuda" "float"  op.create  "_var_"     !
   
    16 2048 *             1 "cpu"  "int"  op.create  "_ids_"     !
    16 2048 *             1 "cpu"  "int"  op.create  "_mask_"    !
    "HEADS_NUM" @ 2048 *  1 "cpu"  "float"  op.create  "_alibi_"   !
    16 2048 2048 * *      1 "cpu"  "float"  op.create  "_xmask_"   !   
%end

%def create_dynamic
    0 #

    $tokens !
    $batch  !

    "_ids_" @ 0  $batch @   $tokens @  2 op.view  "ids_" !

    "_mask_" @ 0  $batch @   $tokens @  2 op.view  "mask_" !

    ;; xinput in GPU, xinput_ in CPU
    {
        "_var_"    @ 0  $batch @   $tokens @  "HIDDEN_SIZE" @ 3 op.view  "xinput" !

        $batch @ $tokens @ "HIDDEN_SIZE" @ * * 
    }
   
    ;; alibi in GPU, alibi_ in CPU
    {
        dup
        "_var_" @ swap  1 "HEADS_NUM" @ 1 $tokens @  4 op.view   "alibi" ! 
        "_alibi_" @ 0   1 "HEADS_NUM" @ 1 $tokens @  4 op.view   "alibi_" ! 
    
        "HEADS_NUM" @ $tokens @ * +  
    }

    ;; xmask in GPU, xmask_ in CPU
    {
        dup
        "_var_" @ swap $batch @ 1 $tokens @  $tokens @  4 op.view   "xmask"  ! 
        "_xmask_" @ 0  $batch @ 1 $tokens @  $tokens @  4 op.view   "xmask_" ! 
    
        $batch @ $tokens @ $tokens @ * * +
    }

    ;; xa, xb, xc, xd, xe
    {
        dup  

        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xa" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "ya" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "za" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        
        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xb" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yb" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zb" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +

        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xc" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yc" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zc" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        
        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xd" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yd" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zd" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        
        dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xe" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "ye" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "ze" !
    }
    
    ;; x3a x3b
    {
        dup

        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x3" @ 3 op.view "x3a" !
        dup $batch @ $tokens @ "HIDDEN_SIZE_x3" @  * * +


        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x3" @ 3 op.view "x3b" !
    }
   
    ;; x4a x4b x5a
    {
        dup

        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x4" @ 3 op.view "x4a" !
        dup $batch @ $tokens @ "HIDDEN_SIZE_x4" @  * * +
 
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x4" @ 3 op.view "x4b" !

        dup 
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x5" @ 3 op.view "x5a" !
    }
 

    ;; output logits 
    {
        dup

        $batch @ $tokens @ "HIDDEN_SIZE" @ 5 * * * +
       
        dup
        "_var_" @ swap "VOCAB_SIZE" @ "HIDDEN_SIZE" @ 2 op.view "lm_head.weight" !
       
        "VOCAB_SIZE" @  "HIDDEN_SIZE" @ * +
    
        "_var_" @ swap 0 "VOCAB_SIZE" @ 2 op.view  "all_logits" !
    }

    ;; larget memroy
    {
        $batch @ $tokens @ "HIDDEN_SIZE" @ 5 * * * +
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ $tokens @ 4 op.view "xll" !
    }
%end

%def layer_forward
    ;; layernorm for input
    "xinput" @ "xb" @ "xc" @ "input_layernorm.weight" @ "input_layernorm.bias" @ "xe" @  "LN_EPS" @  op.layernorm 
    
    ;; get kqv
    {
        "xe" @ "query.weight" @ "query.bias" @ "xa" @ op.linear  
        "xe" @ "key.weight" @ "key.bias" @ "xb" @ op.linear  
        "xe" @ "value.weight" @ "value.bias" @ "xc" @ op.linear  
    }

    ;; transponse
    {
        "ya" @ "zd" @ op.transpos_0213          ;; query -> zd
        "yb" @ "za" @ op.transpos_0213          ;; key -> za
        "yc" @ "zb" @ op.transpos_0213          ;; value -> zb
    }
    
    ;; get query@key
    "zd" @  "za" @  "xll" @ op.querykey 

    ;; added alibi and apply xmask
    "xll" @ "alibi" @ "xll" @ op.add
    "xll" @ "xmask" @ "xll" @ op.add

    ;; do softmax
    "xll" @ "xll" @ op.softmax

    ;; do attention and transpose back
    "xll" @ "zb" @  "zc" @ op.attn          ;; attn -> zc
    "zc" @ "ya" @ op.transpos_0213
  
    ;; do dense
    "xa" @ "dense.weight" @ "dense.bias" @  "xb" @ op.linear
   
    ;; added residual 
    "xb" @ "xinput" @ "xa" @ op.add

    ;; post layernorm
    "xa" @ "xc" @ "xd" @ "post_attention_layernorm.weight" @ "post_attention_layernorm.bias" @ "xb" @  "LN_EPS" @  op.layernorm
    
    ;; MLP
    ;; xa atteion output
    ;; xb passed post layernorm
    {

        ;; 4h dense & glue
        "xb" @ "dense_h_to_4h.weight" @ "dense_h_to_4h.bias" @ "x4b" @ op.linear
        "x4b" @ "x4b" @ op.gelu

        ;; 4h dense
        "x4b" @ "dense_4h_to_h.weight" @ "dense_4h_to_h.bias" @ "xb" @ op.linear

        ;; residual 
        "xa" @ "xb" @ "xa" @ op.add
    }

%end

%def train_0
    ;; receiving alibi and mask
    {
        0 #
        "ids_" @ 0 io.mpi.bcast
        "mask_" @ 0 io.mpi.bcast

        0 #
        "alibi_" @ 0 io.mpi.bcast
        "alibi" @ "alibi_" @ op.copy

        0 #
        "xmask_" @ 0 io.mpi.bcast
        "xmask" @ "xmask_" @ op.copy
    }

    ;; layer 0
    {
        1 sync_layer 0 #
        "xinput_" @ 0 io.mpi.recv
        "xa" @ "xinput_" @ op.copy
     
        "xa" @ "xb" @ "xc" @  "word_embeddings_layernorm.weight" @ "word_embeddings_layernorm.bias" @ "xinput" @ "LN_EPS" @ op.layernorm 
     
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
   
    {
        2 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        3 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        4 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        5 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        6 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        7 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        8 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        9 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        10 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        11 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        12 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        13 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        14 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        15 sync_layer 0 #

        "xinput" @ 1 io.nccl.recv 
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
 
    ;; output layer 
    {
        0 #
        "lm_head.weight" @ "lm_head.weight_" @ op.copy

        "xa" @ 1 io.nccl.recv 
        
        "xa" @ "xb" @ "xc" @ "ln_f.weight" @ "ln_f.bias" @ "xd" @ "LN_EPS" @ op.layernorm
        
        "xd" @  "ids_" @  "mask_" @  "lm_head.weight" @  "all_logits" @  "xa" @  "lm_head.weight_g_" @ op.loss_backward
        
        "xa" @  "ln_f.weight" @ "ln_f.bias" @  "xc" @ "xd" @  "ln_f.weight_g" @ "ln_f.bias_g" @  "xb" @ "LN_EPS" @ op.layernorm_backward

    }
    op.sync

    "xa" @ io.dump
    
    "ln_f.weight_g" @ io.dump
    
    "xb" @ io.dump

%end

%def train_1
    
    ;; receiving alibi & xmask
    {
        0 #
        "ids_" @ 0 io.mpi.bcast
        "mask_" @ 0 io.mpi.bcast

        0 #
        "alibi_" @ 0 io.mpi.bcast
        "alibi" @ "alibi_" @ op.copy

        0 #
        "xmask_" @ 0 io.mpi.bcast
        "xmask" @ "xmask_" @ op.copy
    }
    
    { 
        1 sync_layer 0 # 

        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    { 
        2 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    { 
        3 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }


    { 
        4 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    { 
        5 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    { 
        6 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    { 
        7 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    { 
        8 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    { 
        9 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    { 
        10 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    { 
        11 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        12 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        13 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    {
        14 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    {
        15 sync_layer 0 # 
        
        "xinput" @ 0 io.nccl.recv
        layer_forward
        "xa" @ 0 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }

    op.sync
%end
