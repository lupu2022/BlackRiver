%def create_var
    1 "cuda" "float"  op.create  "_var_"     !
   
    4 2048 *              1 "cpu"  "int"  op.create  "_ids_"     !
    4 2048 *              1 "cpu"  "int"  op.create  "_mask_"    !
    "HEADS_NUM" @ 2048 *  1 "cpu"  "float"  op.create  "_alibi_"   !
    4 2048 2048 * *       1 "cpu"  "float"  op.create  "_xmask_"   !   
%end

%def create_dynamic
    0 #

    $tokens !
    $batch  !

    "_ids_" @ 0  $batch @   $tokens @  2 op.view  "ids_" !

    "_mask_" @ 0  $batch @   $tokens @  2 op.view  "mask_" !

    ;; xinput in GPU, xinput_ in CPU
    {
        "_var_"    @ 0  $batch @   $tokens @  "HIDDEN_SIZE" @ 3 op.view  "xinput" !

        $batch @ $tokens @ "HIDDEN_SIZE" @ * * 
        dup
        
        "_var_" @ swap  $batch @  $tokens @  "HIDDEN_SIZE" @ 3 op.view  "yout_g" !
    
        $batch @ $tokens @ "HIDDEN_SIZE" @ * * + 
    }
  
    ;; alibi in GPU, alibi_ in CPU
    {
        dup
        "_var_" @ swap  1 "HEADS_NUM" @ 1 $tokens @  4 op.view   "alibi" ! 
        "_alibi_" @ 0   1 "HEADS_NUM" @ 1 $tokens @  4 op.view   "alibi_" ! 

        "HEADS_NUM" @ $tokens @ * +  
    }

    ;;  keep var in layer_norm, used for backward
    {
        dup
        "_var_" @ swap "HIDDEN_SIZE" @ 1 op.view  "input_ln_var" !
        "HIDDEN_SIZE" @  + 
 
        dup
        "_var_" @ swap "HIDDEN_SIZE" @ 1 op.view  "post_ln_var" !
        "HIDDEN_SIZE" @  +
    
        dup
        "_var_" @ swap "HIDDEN_SIZE" @ 1 op.view  "ln_mean" !
        "HIDDEN_SIZE" @  +
    }

    ;; xmask in GPU, xmask_ in CPU
    {
        dup
        "_var_" @ swap $batch @ 1 $tokens @  $tokens @  4 op.view   "xmask"  ! 
        "_xmask_" @ 0  $batch @ 1 $tokens @  $tokens @  4 op.view   "xmask_" ! 
    
        $batch @ $tokens @ $tokens @ * * +
    }
    
    ;; xa, xb, xc
    {
        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xa" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "ya" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "za" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        
        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xb" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yb" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zb" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +

        dup dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xc" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yc" !
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zc" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
    }

    ;; xattn_in, xattn_out, kqv,  x4a, x4b, xll
    {
        dup dup 
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xattn_in" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
   
        dup dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xattn_out" !
        "_var_" @ swap $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yattn_out" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        
        
        dup 
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zquery" ! 
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +


        dup 
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zkey"  !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        
        dup 
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zvalue" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +

        dup 
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x4" @ 3 op.view "x4a" !
        $batch @ $tokens @ "HIDDEN_SIZE" @ 4 * * * +
       
        dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE_x4" @ 3 op.view "x4b" !
        $batch @ $tokens @ "HIDDEN_SIZE" @ 4 * * * +
        
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ $tokens @ 4 op.view "xll" !
    }
    
    ;; output logits , shared with attn_in
    {
        dup
        "_var_" @ swap "VOCAB_SIZE" @ "HIDDEN_SIZE" @ 2 op.view "lm_head.weight" !
        "VOCAB_SIZE" @  "HIDDEN_SIZE" @ * +
        
        "_var_" @ swap 0 "VOCAB_SIZE" @ 2 op.view  "all_logits" !
    }
   
%end

%def layer_forward
    ;; layernorm for input
    "xinput" @ "ln_mean" @ "input_ln_var" @ "input_layernorm.weight" @ "input_layernorm.bias" @ "xattn_in" @  "LN_EPS" @  op.layernorm 
   
    ;; get kqv & transpose
    {
        "xattn_in" @ "query.weight" @ "query.bias" @ "xa" @ op.linear  
        "xattn_in" @ "key.weight" @ "key.bias" @ "xb" @ op.linear  
        "xattn_in" @ "value.weight" @ "value.bias" @ "xc" @ op.linear  

        "ya" @ "zquery" @ op.transpos_0213          
        "yb" @ "zkey" @ op.transpos_0213          
        "yc" @ "zvalue" @ op.transpos_0213  
    }

    ;; attention
    {
        ;; get query@key
        "zquery" @  "zkey" @  "xll" @ op.querykey 

        ;; added alibi and apply xmask
        "xll" @ "alibi" @ "xll" @ op.add
        "xll" @ "xmask" @ "xll" @ op.add

        ;; do softmax
        "xll" @ "xll" @ op.softmax

        ;; do attention and transpose back
        "xll" @ "zvalue" @  "zc" @ op.attn          ;; attn -> zc
        "zc" @ "yattn_out" @ op.transpos_0213
    }

    ;; do dense & residual
    "xattn_out" @ "dense.weight" @ "dense.bias" @  "xb" @ op.linear
    "xb" @ "xinput" @ "xa" @ op.add

    ;; post layernorm
    "xa" @ "ln_mean" @ "post_ln_var" @ "post_attention_layernorm.weight" @ "post_attention_layernorm.bias" @ "xb" @  "LN_EPS" @  op.layernorm

    ;; MLP
    {
        ;; xa atteion output
        ;; xb passed post layernorm
        ;; 4h dense & glue
        
        "xb" @ "dense_h_to_4h.weight" @ "dense_h_to_4h.bias" @ "x4a" @ op.linear
        "x4a" @ "x4b" @ op.gelu

        ;; 4h dense
        "x4b" @ "dense_4h_to_h.weight" @ "dense_4h_to_h.bias" @ "xc" @ op.linear

        ;; residual 
        "xa" @ "xc" @ "xa" @ op.add
    }

%end

%def layer_backward
    
    ;;   MLP backward
    {
        "yout_g" @ "x4b" @ "dense_4h_to_h.weight" @ "dense_4h_to_h.bias" @  "x4b" @ "dense_4h_to_h.weight_g" @ "dense_4h_to_h.bias_g" @ op.linear_backward   
 
        "x4b" @ "x4a" @ "x4a" @ op.gelu_backward
        
        "x4a" @ "xb" @  "dense_h_to_4h.weight" @ "dense_h_to_4h.bias" @ "xa" @  "dense_h_to_4h.weight_g" @ "dense_h_to_4h.bias_g" @ op.linear_backward  
    }
   
    ;;  post layernorm backward
   
    "xa" @ "post_attention_layernorm.weight" @ "post_attention_layernorm.bias" @ "post_ln_var" @  "xb" @ "post_attention_layernorm.weight_g" @ "post_attention_layernorm.bias_g" @ "xc" @ "LN_EPS" @  op.layernorm_backward
    
    "xc" @ "yout_g" @ "xc" @ op.add
    "yout_g" @ "xc" @ op.copy

    ;; dense backward
    
    "xc" @ "xattn_out" @ "dense.weight" @ "dense.bias" @ "xa" @ "dense.weight_g" @ "dense.bias_g" @ op.linear_backward

    "post_attention_layernorm.weight_g" @ io.dump 
    "post_attention_layernorm.bias_g" @ io.dump 
    "dense.weight_g" @ io.dump 
    "dense.bias_g" @ io.dump 
    
    ;; attention backward 
    {
        "ya" @ "zb" @ op.transpos_0213

        
    }

%end

%def one_layer
    sync_layer 0 #
    dup

    "xinput" @ swap io.nccl.recv 
    layer_forward
    "xa" @ swap io.nccl.send
    "xinput_" @ "xinput" @ op.copy
%end

%def train_0
    ;; receiving alibi and mask
    {
        0 #
        "ids_" @ 0 io.mpi.bcast
        "mask_" @ 0 io.mpi.bcast

        0 #
        "alibi_" @ 0 io.mpi.bcast
        "alibi" @ "alibi_" @ op.copy

        0 #
        "xmask_" @ 0 io.mpi.bcast
        "xmask" @ "xmask_" @ op.copy
    }

    ;; layer 0
    {
        1 sync_layer 0 #
        "xinput_" @ 0 io.mpi.recv
        "xa" @ "xinput_" @ op.copy
     
        "xa" @ "xb" @ "xc" @  "word_embeddings_layernorm.weight" @ "word_embeddings_layernorm.bias" @ "xinput" @ "LN_EPS" @ op.layernorm 
     
        layer_forward
        "xa" @ 1 io.nccl.send
        "xinput_" @ "xinput" @ op.copy
    }
    
    1 2 one_layer
    1 3 one_layer
    1 4 one_layer
    1 5 one_layer
    1 6 one_layer
    1 7 one_layer
    1 8 one_layer
    1 9 one_layer
    1 10 one_layer
    1 11 one_layer
    1 12 one_layer
    1 13 one_layer
    1 14 one_layer
    1 15 one_layer
    
    ;; output layer 
    {
        0 #
        "lm_head.weight" @ "lm_head.weight_" @ op.copy

        "xa" @ 1 io.nccl.recv 
        
        "xa" @ "ln_mean" @ "post_ln_var" @ "ln_f.weight" @ "ln_f.bias" @ "xb" @ "LN_EPS" @ op.layernorm
        
        "xb" @  "ids_" @  "mask_" @  "lm_head.weight" @  "all_logits" @  "xa" @  "lm_head.weight_g_" @ op.loss_backward
        
        "xa" @  "ln_f.weight" @ "ln_f.bias" @  "post_ln_var" @ "xb" @  "ln_f.weight_g" @ "ln_f.bias_g" @  "xc" @ "LN_EPS" @ op.layernorm_backward
    
        "xc" @ 1 io.nccl.send
    }
    op.sync

        
%end

%def train_1
    
    ;; receiving alibi & xmask
    {
        0 #
        "ids_" @ 0 io.mpi.bcast
        "mask_" @ 0 io.mpi.bcast

        0 #
        "alibi_" @ 0 io.mpi.bcast
        "alibi" @ "alibi_" @ op.copy

        0 #
        "xmask_" @ 0 io.mpi.bcast
        "xmask" @ "xmask_" @ op.copy
    }
    
    0 1 one_layer
    0 2 one_layer
    0 3 one_layer
    0 4 one_layer
    0 5 one_layer
    0 6 one_layer
    0 7 one_layer
    0 8 one_layer
    0 9 one_layer
    0 10 one_layer
    0 11 one_layer
    0 12 one_layer
    0 13 one_layer
    0 14 one_layer
    0 15 one_layer

    {
        15 sync_layer 0 #
        "xinput" @ "xinput_" @ op.copy
        "yout_g" @ 0 io.nccl.recv
        
        layer_forward
        layer_backward
    }

    op.sync

%end
